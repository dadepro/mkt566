---
title: "Exercise: Predict Ad Click Probability (Logistic Regression)"
author: "Davide Proserpio"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
# Set a fixed random seed so results (splits/metrics) are reproducible across runs
set.seed(123)
# Configure how code chunks render (suppress noisy messages, set figure size, etc.)
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.width = 7, fig.height = 5)

# Load packages (install if missing)
# List of required packages for this notebook
pkgs <- c("pROC", "ggplot2", "stargazer")
# Identify packages that aren't installed yet (so students can knit end-to-end)
to_install <- pkgs[!pkgs %in% installed.packages()[,1]]
if (length(to_install) > 0) install.packages(to_install, quiet = TRUE)
# Load all required packages into the session
invisible(lapply(pkgs, library, character.only = TRUE))
```

# Load Data & Explore Baseline Click Rate
```{r load}
# Load the ad-click data (one row per impression) from CSV into a data.frame
df <- read.csv("ad_click_data.csv")

# Inspect the first few rows and summary statistics.
# Quick peek at the first rows to sanity-check columns and types
head(df)
# Summary stats: distribution of numeric vars and levels of factors
summary(df)

```
**Questions**

- What is the dependent variable here, and how is it coded?  
- Which variables are categorical vs numeric predictors?

# Create Train/Test Split (70%/30%)
```{r split}
# Split dataset into train (70%) and test (30%) to evaluate generalization.

# Randomly select ~30% of rows to form the test set (holdout)
test_idx <- sample(nrow(df), size = 0.3 * nrow(df))
# Training set: all rows NOT in the test index
train <- df[-test_idx, ]
# Test set: rows reserved for honest model evaluation
test  <- df[test_idx, ]
```
**Questions**

- Why is it important to separate training and testing sets?  
- What would happen if we trained and evaluated on the same data?

# Fit Logistic Regression
```{r fit}
# Fit a logistic regression to predict clicks using selected predictors.
# Here we use "pages_viewed" and "ad_position" as features.
logit_fit <- glm(
  click ~ pages_viewed + ad_position,
  data = train,
  family = binomial()
)

# Summarize coefficients in a clean format
# Nicely formatted regression table to inspect coefficient signs and magnitudes
stargazer(logit_fit, type = "text", single.row = TRUE, no.space = TRUE)
```
**Questions**

- Interpret the sign of the coefficients. Which variables increase/decrease the odds of clicking?  

# Predict Probabilities & Classify (Threshold = 0.5)
```{r predict}
# Predict probability of clicking on the test set.
test$p_hat <- predict(logit_fit, newdata = test, type = "response")

# Classify each impression as "predicted click" (1) if p >= 0.5, else 0.
test$y_hat <- as.integer(test$p_hat >= 0.5)
# Inspect true label vs predicted probability and classification for a few rows
head(test[, c("click","p_hat","y_hat")])
```
**Questions**

- What does the threshold of 0.5 mean in practice?  
- How would results change if we used 0.3 or 0.7 instead?

# Evaluation Metrics (Accuracy, Precision, Recall, F1)
```{r metrics}
# Confusion matrix elements
# True Positives: predicted click and actually clicked
TP <- sum(test$y_hat==1 & test$click==1, na.rm = TRUE)
# False Positives: predicted click but actually no click
FP <- sum(test$y_hat==1 & test$click==0, na.rm = TRUE)
# False Negatives: predicted no click but actually clicked
FN <- sum(test$y_hat==0 & test$click==1, na.rm = TRUE)
# True Negatives: predicted no click and actually no click
TN <- sum(test$y_hat==0 & test$click==0, na.rm = TRUE)

print(paste("TP:", TP, "FP:", FP, "FN:", FN, "TN:", TN))

# Compute standard metrics
# Accuracy: overall fraction of correct predictions (can be misleading with class imbalance)
accuracy  <- (TP + TN) / (TP + FP + FN + TN)  # overall correctness
# Precision: among predicted clicks, how many were real clicks
precision <- ifelse((TP + FP)==0, NA, TP/(TP+FP))  # quality of positive predictions
# Recall (sensitivity): among real clicks, how many we correctly predicted
recall    <- TP/(TP+FN)  # coverage of actual positives
# F1: harmonic mean of precision and recall (balances the two)
f1        <- ifelse(is.na(precision) | (precision+recall)==0, NA, 2*precision*recall/(precision+recall))  # balance between precision and recall

data.frame(
  Threshold = 0.50,
  Accuracy = round(accuracy, 3),
  Precision = round(precision, 3),
  Recall = round(recall, 3),
  F1 = round(f1, 3)
)
```
**Questions**

- If the business cares more about "catching all potential clickers," which metric should we prioritize?  
- What tradeoff exists between precision and recall?

# ROC Curve
```{r rocplot}
# ROC curve plots tradeoff between True Positive Rate and False Positive Rate

# Build ROC object from true labels and predicted probabilities
roc_obj <- roc(response = test$click, predictor = test$p_hat, quiet = TRUE)
# False Positive Rate (x-axis) derived from specificities (1 - TNR)
fpr <- 1 - roc_obj$specificities   # False Positive Rate
# True Positive Rate (y-axis), also called recall
tpr <- roc_obj$sensitivities       # True Positive Rate

# Plot ROC curve with the random-guess diagonal as a baseline
ggplot(data = data.frame(FPR = fpr, TPR = tpr), aes(x = FPR, y = TPR)) +
  geom_line(size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "ROC Curve", x = "False Positive Rate (FPR)", y = "True Positive Rate (TPR)") +
  theme_minimal(base_size = 13)
```
**Questions**

- Why is the diagonal red line a "random guess" baseline?  
- What does it mean if our curve is very close to that line?

# AUC-ROC
```{r auc}
# AUC: single-number summary of ROC (probability a random positive ranks above a random negative)
auc_val <- as.numeric(auc(roc_obj))
round(auc_val, 3)
```
**Questions**

- What is the maximum possible AUC? What would an AUC of 0.5 mean?

# Interpretability: Odds Ratios (exp(beta))
```{r odds}
# Convert log-odds coefficients into odds ratios for manager-friendly interpretation
or <- exp(coef(logit_fit))
round(or, 3)
```
**Questions**

- If "`ad_positiontop`" has an odds ratio of ~2.5, how should we interpret this?  
- Why might odds ratios be more intuitive for managers than raw coefficients?

